{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this notebook to explore the dataset I created using an LLM which contains features of the items such as: colour, fabric, price_category etc.. \n",
    "\n",
    "I'll be looking at create a vector database from this which can then be searched on for new items. Need to figure out what this looks like exactly.\n",
    "\n",
    "I will also be looking at creating an additional \"style\" vector from the description maybe that captures the vibe of the item. This will be taken from the description and additional features column.\n",
    "\n",
    "Looking at the data now I should I have created more clear options for the categories. Now I have to go through and do some cleaning.\n",
    "\n",
    "Points that I should take into account with final draft of data cleaning using LLMS:\n",
    "- Clear options for categories that it makes sense for: Category, Colour\n",
    "- Use a separate column for colour shade: light, medium, dark\n",
    "- Create a Category (e.g., clothing, footwear, accessories, homeware) and a Sub Catgeory: (Coats & Jackets, Hoodies, T-shirts, boots, sneakers, rings, hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ProductStructuredInfo.csv', index_col=0)\n",
    "df['currency'] = 'GBP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating embeddings for unstructured text data\n",
    "\n",
    "I'm gonna generate embeddings using CLIP for the following data:\n",
    "- Some structured data -> specific category, primary_colour, secondary_colour, pattern, fabric, price_category, fit\n",
    "- The descriptions -> additional features, description\n",
    "- Key words\n",
    "\n",
    "I'll start using the pre-trained CLIP embeddings, to see how well this works. I'll create the embeddings, aggregate them in a way that makes sense then pick a couple items of clothing and see which ones are close to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_for_embeddings = ['fit', 'specific_category', 'primary_colour', 'secondary_colour', 'pattern', 'fabric', 'price_category']\n",
    "\n",
    "def get_structured_encoding_texts(row: pd.Series):\n",
    "\n",
    "    structured_encoding_texts = []\n",
    "\n",
    "    for column in structured_data_for_embeddings:\n",
    "        if type(row[column]) == str:\n",
    "            structured_encoding_texts.append(row[column])\n",
    "\n",
    "    return structured_encoding_texts\n",
    "\n",
    "def get_descriptive_encoding_texts(row: pd.Series):\n",
    "\n",
    "    descriptive_encoding_texts = []\n",
    "\n",
    "    descriptive_encoding_texts += [x for x in row['additional_features'].split(',') if type(x) == str]\n",
    "    descriptive_encoding_texts += row['description']\n",
    "\n",
    "    return descriptive_encoding_texts\n",
    "\n",
    "def get_key_word_encoding_texts(row: pd.Series):\n",
    "\n",
    "    return [x for x in row['key_words'].split(',') if type(x) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "device = \"cpu\"\n",
    "i = 0\n",
    "\n",
    "# Load the pre-trained CLIP model and its tokenizer.\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def get_item_embeddings(row: pd.Series):\n",
    "\n",
    "    # Tokenize the descriptors using CLIP's tokenizer.\n",
    "    structured_text = get_structured_encoding_texts(row)\n",
    "    structured_text_tokens = clip.tokenize(structured_text).to(device)\n",
    "\n",
    "    descriptive_text = get_structured_encoding_texts(row)\n",
    "    descriptive_text_tokens = clip.tokenize(descriptive_text).to(device)\n",
    "\n",
    "    key_word_text = get_structured_encoding_texts(row)\n",
    "    key_word_text_tokens = clip.tokenize(key_word_text).to(device)\n",
    "\n",
    "    # Generate embeddings for each descriptor.\n",
    "    with torch.no_grad():\n",
    "        structured_text_embeddings = model.encode_text(structured_text_tokens)\n",
    "        descriptive_text_embeddings = model.encode_text(descriptive_text_tokens)\n",
    "        key_word_text_embeddings = model.encode_text(key_word_text_tokens)\n",
    "\n",
    "    # Aggregate embeddings by taking the mean (simple average).\n",
    "    item_structured_info_embedding = structured_text_embeddings.mean(dim=0, keepdim=True)\n",
    "    item_descriptive_info_embedding = descriptive_text_embeddings.mean(dim=0, keepdim=True)\n",
    "    item_key_word_info_embedding = key_word_text_embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Optionally, normalize the aggregated embedding.\n",
    "    item_structured_info_embedding = item_structured_info_embedding / item_structured_info_embedding.norm(dim=-1, keepdim=True)\n",
    "    item_descriptive_info_embedding = item_descriptive_info_embedding / item_descriptive_info_embedding.norm(dim=-1, keepdim=True)\n",
    "    item_key_word_info_embedding = item_key_word_info_embedding / item_key_word_info_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return item_structured_info_embedding, item_descriptive_info_embedding, item_key_word_info_embedding\n",
    "\n",
    "def get_combined_embeddings(row: pd.Series, weights=None):\n",
    "\n",
    "    item_structured_info_embedding, item_descriptive_info_embedding, item_key_word_info_embedding = get_item_embeddings(row)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "    # Ensure weights sum to 1\n",
    "    total_weight = sum(weights)\n",
    "\n",
    "    if total_weight != 0:\n",
    "        weights = [w / total_weight for w in weights]\n",
    "\n",
    "    # Combine the embeddings using the specified weights.\n",
    "    combined_embedding = (weights[0] * item_structured_info_embedding +\n",
    "                          weights[1] * item_descriptive_info_embedding +\n",
    "                          weights[2] * item_key_word_info_embedding)\n",
    "    \n",
    "    # Normalize the combined embedding.\n",
    "    combined_embedding = combined_embedding / combined_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for all items.\n",
    "embeddings_list = [get_combined_embeddings(row) for _, row in df.iterrows()]\n",
    "\n",
    "embeddings = torch.cat(embeddings_list, dim=0)  # Shape: (num_items, embedding_dim)\n",
    "\n",
    "# Since embeddings are normalized, cosine similarity is just the dot product.\n",
    "cosine_similarity_matrix = embeddings @ embeddings.T\n",
    "\n",
    "# Replace the nan cosine similarity values with 0\n",
    "cosine_similarity_matrix = torch.nan_to_num(cosine_similarity_matrix, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 50 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/0/20-09-2024-JW_V66186-P117_1_1.jpg) is similar to items: [(5187, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/6/26-04-2024-JC_1104268648_1_1.jpg'), (12344, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/2/02-08-2024-BLR_782943770001_1_1.jpg'), (6241, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/1/11-11-2022_LLx_541580_1_1.jpg')]\n",
      "Item 100 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/8/28-08-2024-JC_001CSK801007M-GRN_1_1.jpg) is similar to items: [(5793, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/9/09-08-23-JF_GMP01220-P001203-35863_1_1.jpg'), (4548, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/3/0/30-01-2024-JC_D-SH158-LI002-PC_1_1.jpg'), (81, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/3/03-09-2024-JW_001ZPK801004M-YEL_1_1.jpg')]\n",
      "Item 1000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/4/14-05-2024-BLR_USW003-730-951_1_1.jpg) is similar to items: [(2307, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/1/01-05-2024-JC_SS24SS01GRV_1_1.jpg'), (960, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/4/14-05-2024-BLR_USW203-730-951_1_1.jpg'), (930, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/9/19-09-24-AJ_UTS024-726-4013_1_1.jpg')]\n",
      "Item 2000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/9/09-04-2024-JD_3824-0081-803-20_1_1.jpg) is similar to items: [(6419, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/5/15-04-2024-BLR_1141530-WHF_2_1.jpg'), (7376, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/1/01-03-2024-LB_29525-WH_1_1.jpg'), (6442, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/5/15-03-2024-LB_1123157-VLW_2_1.jpg')]\n",
      "Item 3000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/1/11-09-2024-LB_I033937-89XX_1_1.jpg) is similar to items: [(918, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/2/22-09-2023-JWx_178317-03_1_1.jpg'), (2114, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/0/10-09-2024-LB_B24367-BLK_m1_1.jpg'), (340, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/0/20-12-2023-LB_AAPSWMA348XXL-BKX_1_1.jpg')]\n"
     ]
    }
   ],
   "source": [
    "top_k = 3  # Adjust this to see more similar items.\n",
    "for i in [50, 100, 1000, 2000, 3000]:\n",
    "    # Get similarity scores for the i-th item.\n",
    "    sim_scores = cosine_similarity_matrix[i]\n",
    "    # Sort indices in descending order of similarity.\n",
    "    # Exclude the item itself (which will have a similarity of 1.0).\n",
    "    similar_indices = torch.topk(sim_scores, top_k + 1).indices.cpu().numpy()\n",
    "    similar_indices = [idx for idx in similar_indices if idx != i][:top_k]\n",
    "    print(f\"Item {i} (descriptors: {df.iloc[i]['image_urls'].split(\"'\")[1]}) is similar to items: {[(idx, df.iloc[idx]['image_urls'].split(\"'\")[1]) for idx in similar_indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for all items. Only looking at the descriptive embeddings and the key word embeddings.\n",
    "embeddings_list = [get_combined_embeddings(row, weights=[0, 0.5, 0.5]) for _, row in df.iterrows()]\n",
    "\n",
    "embeddings = torch.cat(embeddings_list, dim=0)  # Shape: (num_items, embedding_dim)\n",
    "\n",
    "# Since embeddings are normalized, cosine similarity is just the dot product.\n",
    "cosine_similarity_matrix = embeddings @ embeddings.T\n",
    "\n",
    "# Replace the nan cosine similarity values with 0\n",
    "cosine_similarity_matrix = torch.nan_to_num(cosine_similarity_matrix, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 50 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/0/20-09-2024-JW_V66186-P117_1_1.jpg) is similar to items: [(5187, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/6/26-04-2024-JC_1104268648_1_1.jpg'), (12344, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/2/02-08-2024-BLR_782943770001_1_1.jpg'), (6241, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/1/11-11-2022_LLx_541580_1_1.jpg')]\n",
      "Item 100 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/8/28-08-2024-JC_001CSK801007M-GRN_1_1.jpg) is similar to items: [(5793, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/9/09-08-23-JF_GMP01220-P001203-35863_1_1.jpg'), (4548, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/3/0/30-01-2024-JC_D-SH158-LI002-PC_1_1.jpg'), (81, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/3/03-09-2024-JW_001ZPK801004M-YEL_1_1.jpg')]\n",
      "Item 1000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/4/14-05-2024-BLR_USW003-730-951_1_1.jpg) is similar to items: [(2307, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/1/01-05-2024-JC_SS24SS01GRV_1_1.jpg'), (960, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/4/14-05-2024-BLR_USW203-730-951_1_1.jpg'), (930, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/9/19-09-24-AJ_UTS024-726-4013_1_1.jpg')]\n",
      "Item 2000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/9/09-04-2024-JD_3824-0081-803-20_1_1.jpg) is similar to items: [(6419, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/5/15-04-2024-BLR_1141530-WHF_2_1.jpg'), (7376, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/1/01-03-2024-LB_29525-WH_1_1.jpg'), (6442, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/5/15-03-2024-LB_1123157-VLW_2_1.jpg')]\n",
      "Item 3000 (descriptors: https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/1/11-09-2024-LB_I033937-89XX_1_1.jpg) is similar to items: [(918, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/2/22-09-2023-JWx_178317-03_1_1.jpg'), (2114, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/1/0/10-09-2024-LB_B24367-BLK_m1_1.jpg'), (340, 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/0/20-12-2023-LB_AAPSWMA348XXL-BKX_1_1.jpg')]\n"
     ]
    }
   ],
   "source": [
    "top_k = 3  # Adjust this to see more similar items.\n",
    "for i in [50, 100, 1000, 2000, 3000]:\n",
    "    # Get similarity scores for the i-th item.\n",
    "    sim_scores = cosine_similarity_matrix[i]\n",
    "    # Sort indices in descending order of similarity.\n",
    "    # Exclude the item itself (which will have a similarity of 1.0).\n",
    "    similar_indices = torch.topk(sim_scores, top_k + 1).indices.cpu().numpy()\n",
    "    similar_indices = [idx for idx in similar_indices if idx != i][:top_k]\n",
    "    print(f\"Item {i} (descriptors: {df.iloc[i]['image_urls'].split(\"'\")[1]}) is similar to items: {[(idx, df.iloc[idx]['image_urls'].split(\"'\")[1]) for idx in similar_indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list_structured = [get_combined_embeddings(row, weights=[1, 0, 0]) for _, row in df.iterrows()]\n",
    "embeddings_list_descriptive = [get_combined_embeddings(row, weights=[0, 1, 0]) for _, row in df.iterrows()]\n",
    "embeddings_list_keyword = [get_combined_embeddings(row, weights=[0, 0, 1]) for _, row in df.iterrows()]\n",
    "\n",
    "df['structured_embeddings'] = [embeddings_list_structured[i].cpu().numpy() for i in range(len(embeddings_list_structured))]\n",
    "df['descriptive_embeddings'] = [embeddings_list_descriptive[i].cpu().numpy() for i in range(len(embeddings_list_descriptive))]\n",
    "df['keyword_embeddings'] = [embeddings_list_keyword[i].cpu().numpy() for i in range(len(embeddings_list_keyword))]\n",
    "\n",
    "df.to_csv('ProductStructuredInfoWithEmbeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list_all = [get_combined_embeddings(row) for _, row in df.iterrows()]\n",
    "embeddings_list_split_descriptive_keyword = [get_combined_embeddings(row, weights=[0, 0.5, 0.5]) for _, row in df.iterrows()]\n",
    "embeddings_list_descriptive = [get_combined_embeddings(row, weights=[0, 1, 0]) for _, row in df.iterrows()]\n",
    "embeddings_list_keyword = [get_combined_embeddings(row, weights=[0, 0, 1]) for _, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.8361e-03, -7.3256e-03, -6.2675e-04,  2.3367e-02,  8.4577e-03,\n",
       "         -1.2771e-02, -1.5162e-02, -1.2694e-01,  2.1795e-02,  8.4503e-03,\n",
       "         -5.5618e-03, -1.8069e-02, -2.3803e-02, -8.9944e-03, -1.6894e-03,\n",
       "          2.1849e-02,  2.7212e-02,  7.1085e-03,  1.2457e-03,  7.0266e-03,\n",
       "          1.4086e-02, -1.0944e-02,  1.2900e-02,  4.0064e-03, -2.2470e-02,\n",
       "         -7.8123e-04, -2.0724e-02, -2.2986e-03,  4.3670e-03,  1.1829e-02,\n",
       "         -4.5569e-03, -6.4352e-03,  2.2341e-03,  3.0442e-02, -5.1545e-03,\n",
       "          1.0208e-02,  7.1234e-03,  1.6434e-02, -4.0992e-04,  5.3224e-03,\n",
       "         -1.9843e-02, -1.4645e-02,  4.0405e-03, -1.6330e-02,  2.0935e-02,\n",
       "          1.7120e-02, -8.6162e-03, -3.6416e-03,  1.7573e-02,  1.9455e-03,\n",
       "          2.1454e-03, -2.2090e-02,  8.9318e-03, -6.2192e-03,  7.1635e-03,\n",
       "         -1.6911e-02, -2.7450e-03,  2.5696e-03, -2.9815e-02,  1.3900e-02,\n",
       "          6.0852e-03, -3.1406e-02,  1.3249e-02, -1.2197e-02, -3.0038e-03,\n",
       "         -1.4968e-02, -1.5483e-02, -3.7850e-02,  3.0392e-03, -1.4579e-02,\n",
       "          8.2273e-03, -8.3275e-03, -1.9321e-02,  1.8299e-02,  2.9886e-03,\n",
       "          1.7697e-04,  2.5408e-02, -1.1766e-02, -1.4463e-02, -2.3502e-02,\n",
       "         -2.2103e-02, -4.5030e-03, -2.4399e-02,  2.8824e-02,  2.9507e-03,\n",
       "         -3.4506e-03,  9.8056e-03, -2.6101e-02,  1.0787e-02, -1.6996e-03,\n",
       "         -5.8133e-03, -1.2257e-02, -1.6685e-01,  2.4309e-02, -2.1720e-02,\n",
       "          1.0287e-02,  1.5164e-02,  3.9013e-03,  3.7448e-03, -2.8973e-04,\n",
       "          1.9464e-02, -1.0680e-03,  1.4643e-02,  1.5257e-03, -1.4003e-02,\n",
       "         -1.2853e-02,  1.0735e-02,  1.0386e-02, -8.3504e-03, -3.7901e-03,\n",
       "          9.2868e-03,  2.1939e-02, -1.8023e-02,  4.1718e-03,  4.9075e-03,\n",
       "         -2.1490e-02,  1.1500e-02,  2.6304e-02,  4.6365e-03,  2.6794e-02,\n",
       "          9.6081e-04, -3.4173e-02, -7.3647e-03,  5.9589e-03, -4.2464e-03,\n",
       "         -7.7204e-03, -1.5535e-02, -5.3936e-03,  8.5937e-03,  1.5832e-02,\n",
       "          2.3482e-02,  1.9744e-02, -4.1262e-03,  6.2981e-01,  1.5538e-02,\n",
       "          1.1737e-02, -1.5822e-03, -1.9165e-02, -5.1597e-03,  3.1422e-03,\n",
       "          1.4022e-04, -8.3153e-03, -5.7723e-03,  3.7811e-03, -2.0742e-02,\n",
       "          4.8038e-03, -3.2521e-03, -4.9048e-04, -6.2053e-03, -1.3139e-02,\n",
       "          1.3306e-02, -8.5451e-04,  7.5283e-04,  1.1375e-02, -5.1036e-03,\n",
       "         -5.4278e-03, -2.5542e-02,  1.9078e-02, -7.5636e-03,  1.0921e-02,\n",
       "          8.6879e-03,  2.1661e-03, -8.1446e-03, -4.8039e-03, -5.0698e-04,\n",
       "         -2.9250e-03, -2.2020e-03, -1.0494e-02,  1.2714e-02, -1.2449e-02,\n",
       "         -2.1945e-03,  5.8177e-03, -1.2072e-02,  5.5884e-03, -1.8178e-02,\n",
       "          1.8934e-02, -1.8495e-02, -3.2801e-03,  9.2801e-03, -6.0810e-03,\n",
       "          8.7470e-03,  2.8545e-02,  8.4083e-03, -3.3222e-03, -1.8850e-02,\n",
       "         -6.2000e-03,  1.2053e-02, -2.2891e-02,  1.5870e-03,  2.2371e-03,\n",
       "          1.4941e-02, -8.6715e-03,  5.5718e-03,  1.3397e-02, -7.8886e-03,\n",
       "         -1.3998e-02, -7.8671e-03,  6.2011e-03, -1.5919e-02, -1.0648e-02,\n",
       "          1.1739e-02,  1.4161e-02,  1.6214e-02,  1.7129e-02, -4.1247e-03,\n",
       "          8.7947e-03, -3.2444e-03, -1.4291e-02,  5.5120e-03,  4.1521e-03,\n",
       "          4.0143e-02,  2.8797e-02, -1.2526e-02, -1.8790e-03, -2.6105e-02,\n",
       "         -9.0881e-03,  9.2574e-03, -4.2143e-04, -2.6989e-02, -1.7183e-02,\n",
       "         -2.9051e-02, -1.6115e-02, -3.9564e-03, -7.2624e-03,  2.1962e-02,\n",
       "          5.1477e-03,  1.0845e-02,  2.1537e-02,  8.4887e-03,  1.1063e-04,\n",
       "          1.5600e-02,  5.2598e-04,  1.2859e-02, -1.6764e-02,  4.0141e-03,\n",
       "          6.1689e-03,  2.0135e-02, -5.1528e-04,  1.4779e-03, -2.0939e-02,\n",
       "         -2.2440e-03,  5.2404e-03,  7.0570e-03,  6.3014e-03, -4.7879e-03,\n",
       "         -4.3425e-03,  4.6258e-03, -8.3480e-03, -1.0432e-02,  6.4474e-03,\n",
       "         -2.7336e-02,  5.7799e-03,  2.9047e-03,  2.3886e-02, -2.6932e-02,\n",
       "          2.4167e-02,  3.3999e-02, -2.4044e-02,  1.2298e-02,  4.5888e-03,\n",
       "          1.4268e-03,  5.3903e-03, -1.1664e-03, -2.7677e-03,  9.1243e-03,\n",
       "         -9.2002e-03, -5.1440e-03, -2.0483e-02,  7.7870e-05,  4.4442e-03,\n",
       "          3.0841e-03,  5.7127e-03,  1.0083e-02, -3.6479e-02,  9.7139e-03,\n",
       "         -6.8190e-03,  8.4811e-04, -1.4483e-03,  9.7228e-03,  5.7407e-04,\n",
       "          2.8891e-02,  9.6718e-03,  1.1942e-02,  5.8384e-03, -3.4935e-03,\n",
       "          1.5163e-02,  1.7960e-02, -7.6386e-03,  1.8755e-03,  1.2618e-03,\n",
       "         -1.8992e-03, -1.6356e-02,  1.6040e-02,  2.0924e-02, -1.0801e-03,\n",
       "         -7.1598e-03, -3.1118e-02, -8.2461e-03,  1.6921e-02,  1.0938e-02,\n",
       "          1.0216e-02,  8.8795e-03, -1.1928e-02,  4.0806e-03,  1.3190e-02,\n",
       "          1.6372e-02,  4.2080e-03,  1.2202e-02,  4.4527e-03,  1.2244e-02,\n",
       "          1.3156e-03, -2.1942e-03,  6.2895e-01,  8.0057e-03, -1.0926e-02,\n",
       "          3.4326e-03, -3.4233e-03,  1.6646e-03,  1.1869e-02,  3.6594e-03,\n",
       "          1.4667e-02,  4.9448e-02,  1.5184e-02,  1.0356e-02,  4.5489e-03,\n",
       "          5.3871e-04, -8.2452e-03,  1.0565e-03,  1.2255e-02, -2.6316e-01,\n",
       "          7.6029e-03,  2.0747e-02,  6.7823e-03,  2.0289e-05, -8.8769e-03,\n",
       "          5.6949e-03,  1.8907e-02, -7.6442e-04, -2.5739e-03,  5.1563e-03,\n",
       "         -7.2940e-03, -1.5078e-02,  5.0154e-03,  9.4069e-04,  4.6835e-03,\n",
       "         -2.4871e-02,  2.5938e-03,  1.6866e-02,  5.4761e-03, -1.5317e-03,\n",
       "          1.5228e-02,  7.6814e-03,  4.4577e-03,  9.5336e-03,  1.9684e-02,\n",
       "          5.9228e-03, -9.9955e-03,  7.6132e-03,  1.0801e-03,  9.4173e-04,\n",
       "          1.1051e-04, -1.2414e-02,  1.9345e-02,  1.1763e-02,  2.3782e-02,\n",
       "         -1.9884e-02,  4.9423e-03,  1.2926e-02,  2.0671e-03,  1.4438e-02,\n",
       "         -4.0513e-03, -4.3833e-03, -6.7140e-03,  7.7261e-03,  1.0673e-03,\n",
       "          7.5286e-04, -1.5028e-02,  1.2974e-02, -1.9217e-02,  2.1399e-02,\n",
       "         -2.0532e-02, -4.5704e-03, -2.3577e-03,  1.0577e-03,  1.5479e-02,\n",
       "         -1.0339e-02,  1.9789e-03,  7.5336e-03, -4.9780e-03,  8.9703e-03,\n",
       "         -1.4159e-02, -2.0890e-02,  6.2103e-03,  2.1423e-02, -1.2404e-03,\n",
       "          9.7657e-03, -7.3808e-03,  8.8440e-03, -2.2913e-02,  5.9025e-02,\n",
       "          3.2791e-03,  2.2083e-03,  1.2334e-02, -2.1819e-03, -2.3428e-02,\n",
       "         -5.4273e-04,  7.6588e-03, -1.1522e-02,  1.4769e-02, -1.7588e-02,\n",
       "          2.3112e-03, -2.5621e-03,  1.3536e-02,  9.7077e-03,  1.1522e-02,\n",
       "         -5.0176e-03, -1.1577e-02, -1.2365e-02,  5.4404e-03,  7.0262e-03,\n",
       "          8.8419e-03, -6.3673e-03, -4.1827e-03, -3.2146e-03,  1.2686e-02,\n",
       "          6.5787e-03, -1.2085e-02, -5.6313e-03, -9.2296e-03,  1.8253e-02,\n",
       "          4.8349e-03, -3.1031e-03, -5.7479e-03,  1.4096e-02,  6.2292e-03,\n",
       "         -3.4331e-03, -1.4947e-02,  2.5329e-03, -2.3424e-02,  1.5993e-02,\n",
       "         -1.8729e-02,  9.5902e-04,  9.9010e-03, -1.5308e-02,  1.9059e-02,\n",
       "          8.4242e-03, -1.8106e-02, -1.4005e-02,  8.5220e-03, -4.2802e-03,\n",
       "         -1.4944e-02,  1.4251e-02, -1.3285e-02,  4.6638e-03,  4.2175e-03,\n",
       "         -8.2603e-05,  5.4645e-03,  1.1003e-02, -6.6411e-03, -1.3767e-02,\n",
       "         -1.6001e-03,  1.2407e-02, -2.5771e-03,  1.7083e-02,  8.8997e-03,\n",
       "         -1.1859e-02, -4.1597e-03, -7.4650e-03, -1.0171e-02, -2.1579e-02,\n",
       "          1.2026e-02,  1.2479e-05,  5.5663e-03, -5.3100e-04,  6.8335e-03,\n",
       "          1.2011e-02,  1.6520e-02,  1.3864e-02,  1.5919e-02,  1.0829e-02,\n",
       "          1.2997e-02, -7.1535e-02,  2.0005e-03, -5.8614e-03, -3.7235e-03,\n",
       "         -2.6390e-03,  5.8702e-03,  7.0280e-03,  6.1675e-03, -1.6400e-02,\n",
       "          5.1775e-03,  2.3918e-03, -2.0716e-03,  2.9377e-02, -5.2712e-03,\n",
       "          5.0218e-03,  5.6714e-03,  9.3674e-04, -1.7206e-02,  9.1691e-03,\n",
       "          7.5948e-03, -1.1750e-02,  1.2354e-02,  9.4517e-03,  6.2961e-03,\n",
       "          1.8997e-03, -1.5257e-02, -1.4309e-02,  2.3274e-03, -2.4462e-02,\n",
       "         -6.4793e-03,  1.0664e-02]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_list_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_list_descriptive[0] @ embeddings_list_keyword[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_embeddings(row: pd.Series, weights=None):\n",
    "\n",
    "    item_structured_info_embedding, item_descriptive_info_embedding, item_key_word_info_embedding = get_item_embeddings(row)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "    # Ensure weights sum to 1\n",
    "    total_weight = sum(weights)\n",
    "\n",
    "    if total_weight != 0:\n",
    "        weights = [w / total_weight for w in weights]\n",
    "\n",
    "    \n",
    "\n",
    "    # Combine the embeddings using the specified weights.\n",
    "    combined_embedding = (weights[0] * item_structured_info_embedding +\n",
    "                          weights[1] * item_descriptive_info_embedding +\n",
    "                          weights[2] * item_key_word_info_embedding)\n",
    "    \n",
    "    # Normalize the combined embedding.\n",
    "    combined_embedding = combined_embedding / combined_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ImageEmbeddings.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.021941</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>-0.006881</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.027262</td>\n",
       "      <td>-0.041029</td>\n",
       "      <td>0.035903</td>\n",
       "      <td>0.056407</td>\n",
       "      <td>0.022258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.023353</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>-0.012616</td>\n",
       "      <td>-0.035318</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>-0.008665</td>\n",
       "      <td>0.040466</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.041450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.034141</td>\n",
       "      <td>-0.036617</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>-0.042607</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.060865</td>\n",
       "      <td>-0.004280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031366</td>\n",
       "      <td>-0.021499</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>-0.013308</td>\n",
       "      <td>-0.027678</td>\n",
       "      <td>-0.005887</td>\n",
       "      <td>-0.002814</td>\n",
       "      <td>0.073143</td>\n",
       "      <td>-0.007298</td>\n",
       "      <td>0.063920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.054976</td>\n",
       "      <td>0.037239</td>\n",
       "      <td>0.068838</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026102</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.044905</td>\n",
       "      <td>-0.007300</td>\n",
       "      <td>-0.027224</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>-0.018067</td>\n",
       "      <td>0.056675</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.050511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.041346</td>\n",
       "      <td>0.016351</td>\n",
       "      <td>-0.003159</td>\n",
       "      <td>-0.028911</td>\n",
       "      <td>-0.019515</td>\n",
       "      <td>-0.049184</td>\n",
       "      <td>-0.006748</td>\n",
       "      <td>0.053963</td>\n",
       "      <td>-0.022850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028399</td>\n",
       "      <td>-0.009819</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>-0.029098</td>\n",
       "      <td>-0.011516</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>-0.003234</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.038571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007424</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.032492</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>-0.073587</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.058955</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014333</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.041731</td>\n",
       "      <td>-0.009086</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>-0.007801</td>\n",
       "      <td>0.073651</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>0.064285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>-0.009869</td>\n",
       "      <td>-0.003345</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>-0.069728</td>\n",
       "      <td>0.043092</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046293</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.043155</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>-0.028547</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>-0.018724</td>\n",
       "      <td>0.062319</td>\n",
       "      <td>-0.004940</td>\n",
       "      <td>0.048564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007601</td>\n",
       "      <td>0.019003</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>-0.010884</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>-0.054022</td>\n",
       "      <td>0.038994</td>\n",
       "      <td>0.031059</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>-0.008517</td>\n",
       "      <td>-0.025109</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>0.047735</td>\n",
       "      <td>0.021587</td>\n",
       "      <td>0.060495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>0.035614</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>-0.019514</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.032842</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052601</td>\n",
       "      <td>-0.009565</td>\n",
       "      <td>-0.006058</td>\n",
       "      <td>-0.018826</td>\n",
       "      <td>-0.031022</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.068595</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.029575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.008593</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>0.020832</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.021305</td>\n",
       "      <td>-0.006184</td>\n",
       "      <td>-0.016481</td>\n",
       "      <td>0.031014</td>\n",
       "      <td>0.043140</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056304</td>\n",
       "      <td>-0.009199</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.074225</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.021653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005505</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>-0.000938</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>-0.036148</td>\n",
       "      <td>0.047710</td>\n",
       "      <td>0.043861</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036777</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>-0.014275</td>\n",
       "      <td>-0.041193</td>\n",
       "      <td>-0.007958</td>\n",
       "      <td>-0.009339</td>\n",
       "      <td>0.051277</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.045975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.021941  0.007341  0.012450 -0.006881  0.002544  0.027262 -0.041029   \n",
       "1  0.009438  0.016171  0.000421  0.034141 -0.036617  0.013452 -0.042607   \n",
       "2  0.021674  0.027032  0.018431 -0.002317 -0.002001  0.007704 -0.054976   \n",
       "3 -0.000614  0.041346  0.016351 -0.003159 -0.028911 -0.019515 -0.049184   \n",
       "4  0.007424  0.035297  0.032492  0.007850 -0.008046  0.009427 -0.073587   \n",
       "5  0.019975  0.035504  0.017329 -0.009869 -0.003345  0.022369 -0.069728   \n",
       "6  0.007601  0.019003  0.021572  0.014649 -0.010884  0.010585 -0.054022   \n",
       "7 -0.023843 -0.005564  0.035614 -0.005475 -0.037241  0.001322 -0.019514   \n",
       "8 -0.008593 -0.006366  0.020832  0.014902 -0.021305 -0.006184 -0.016481   \n",
       "9 -0.005505  0.028986 -0.000938  0.013149 -0.044339  0.013578 -0.036148   \n",
       "\n",
       "        7         8         9    ...       502       503       504       505  \\\n",
       "0  0.035903  0.056407  0.022258  ... -0.009007 -0.023353  0.010537 -0.012616   \n",
       "1  0.002757  0.060865 -0.004280  ... -0.031366 -0.021499  0.003531 -0.013308   \n",
       "2  0.037239  0.068838  0.010605  ... -0.026102  0.001598  0.044905 -0.007300   \n",
       "3 -0.006748  0.053963 -0.022850  ... -0.028399 -0.009819  0.003378 -0.029098   \n",
       "4  0.020023  0.058955  0.005529  ... -0.014333 -0.001263  0.041731 -0.009086   \n",
       "5  0.043092  0.055619  0.015073  ... -0.046293  0.006483  0.043155 -0.002985   \n",
       "6  0.038994  0.031059  0.004289  ... -0.020539  0.005201  0.036903 -0.008517   \n",
       "7  0.006147  0.032842  0.010656  ... -0.052601 -0.009565 -0.006058 -0.018826   \n",
       "8  0.031014  0.043140  0.004957  ... -0.056304 -0.009199  0.009224 -0.026027   \n",
       "9  0.047710  0.043861  0.030804  ... -0.036777  0.016647  0.034929 -0.014275   \n",
       "\n",
       "        506       507       508       509       510       511  \n",
       "0 -0.035318  0.004427 -0.008665  0.040466 -0.000854  0.041450  \n",
       "1 -0.027678 -0.005887 -0.002814  0.073143 -0.007298  0.063920  \n",
       "2 -0.027224 -0.012658 -0.018067  0.056675  0.020806  0.050511  \n",
       "3 -0.011516 -0.006054 -0.003234  0.058020 -0.000327  0.038571  \n",
       "4 -0.024412 -0.010870 -0.007801  0.073651  0.003466  0.064285  \n",
       "5 -0.028547  0.010435 -0.018724  0.062319 -0.004940  0.048564  \n",
       "6 -0.025109  0.008413 -0.005523  0.047735  0.021587  0.060495  \n",
       "7 -0.031022  0.004995  0.003231  0.068595  0.023699  0.029575  \n",
       "8 -0.042306  0.004781  0.003418  0.074225  0.019379  0.021653  \n",
       "9 -0.041193 -0.007958 -0.009339  0.051277  0.020266  0.045975  \n",
       "\n",
       "[10 rows x 512 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
