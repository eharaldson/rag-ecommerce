{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"/Users/erikharaldson/Documents/FindAI/DataSets/\"\n",
    "image_directory = \"/Users/erikharaldson/Documents/FindAI/Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_info = pd.read_csv(dataset_directory + \"ProductStructuredInfo.csv\")\n",
    "descriptive_embeddings = pd.read_csv(dataset_directory + \"DescriptiveEmbeddings.csv\", header=None)\n",
    "keyword_embeddings = pd.read_csv(dataset_directory + \"KeywordEmbeddings.csv\", header=None)\n",
    "image_embeddings = pd.read_csv(dataset_directory + \"ImageEmbeddings.csv\", header=None)\n",
    "structured_embeddings = pd.read_csv(dataset_directory + \"StructuredEmbeddings.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_embeddings_torch = torch.tensor(descriptive_embeddings.values, dtype=torch.float32)\n",
    "image_embeddings_torch = torch.tensor(image_embeddings.values, dtype=torch.float32)\n",
    "keyword_embeddings_torch = torch.tensor(keyword_embeddings.values, dtype=torch.float32)\n",
    "structured_embeddings_torch = torch.tensor(structured_embeddings.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "# Test for text retrieval\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load the pre-trained CLIP model and its tokenizer.\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "text = \"navy\"\n",
    "\n",
    "text_tokens = clip.tokenize(text).to(device)\n",
    "text_embeddings = model.encode_text(text_tokens)\n",
    "text_embeddings = text_embeddings.mean(dim=0, keepdim=True)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "text_embeddings = text_embeddings.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Test for image retrieval\n",
    "image = Image.open(image_directory + \"coatjpeg.jpg\")\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "img_embedding = image_features.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embeddings(embedding1, embedding2):\n",
    "    \n",
    "    # Check if embeddings have the same shape\n",
    "    if embedding1.shape != embedding2.shape:\n",
    "        raise ValueError(f\"Embedding shapes don't match: {embedding1.shape} vs {embedding2.shape}\")\n",
    "    \n",
    "    # Calculate the average embedding\n",
    "    average_embedding = (embedding1 + embedding2) / 2\n",
    "    \n",
    "    return average_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix text and image embeddings\n",
    "mixed_embeddings = average_embeddings(text_embeddings, img_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image_urls_top_k(product_info: pd.DataFrame, input_embedding, embedding_database, top_k = 3):\n",
    "\n",
    "    cosine_similarity_matrix = input_embedding @ embedding_database.T\n",
    "    cosine_similarity_matrix = torch.nan_to_num(cosine_similarity_matrix, nan=0.0)\n",
    "\n",
    "    similar_indices = torch.topk(cosine_similarity_matrix, top_k).indices.cpu().numpy()\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    for indx in similar_indices:\n",
    "\n",
    "        first_url = product_info.iloc[indx][\"image_urls\"].split(\"'\")[1]\n",
    "        image_urls.append(first_url)\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/3/23-09-24-LS_WN-J002-051-1_1_1.jpg',\n",
       " 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/0/8/08-08-2023-JA_232-020206-7228-900_1_1.jpg',\n",
       " 'https://media.endclothing.com/media/f_auto,q_auto:eco,w_200/prodmedia/media/catalog/product/2/4/24-06-24-ns2_242-020219-9223-900_1.jpg']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_image_urls_top_k(product_info=df_item_info, input_embedding=mixed_embeddings, embedding_database=image_embeddings_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
